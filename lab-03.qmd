---
title: "lab 3: COVID-19"
subtitle: 'Ecosystem Science and Sustainability 330'
author:
  - name: Megan Hoover
    email: megan.hoover21@gmail.com
format: html
execute: 
  echo: true
output:
  html
    self contained:true
knitr:
  opts_chunk:
    collapse: true
    comment: "#>"
---

## Q2: Lets pretend it in January 1st, 2022. You are a data scientist for the state of Colorado Department of Public Health.

You’ve been tasked with giving a report to Governor Polis each morning about the most current COVID-19 conditions at the county level.

As it stands, the Colorado Department of Public Health maintains a watch list of counties that are being monitored for worsening corona virus trends. There are six criteria used to place counties on the watch list:

1.Doing fewer than 150 tests per 100,000 residents daily (over a 7-day average)
2.More than 100 new cases per 100,000 residents over the past 14 days…
3.25 new cases per 100,000 residents and an 8% test positivity rate
4.10% or greater increase in COVID-19 hospitalized patients over the past 3 days
5.Fewer than 20% of ICU beds available
6.Fewer than 25% ventilators available 
Of these 6 conditions, you are in charge of monitoring condition number 2.

To do this job well, you should set up a reproducible framework to communicate the following in a way that can be updated every time new data is released (daily):

1. cumulative cases in the 5 worst counties
2. total NEW cases in the 5 worst counties
3. A list of safe counties
4. A text report describing the total new cases, total cumulative cases, and number of safe counties.
You should build this analysis in such a way that running it will extract the most current data straight from the NY-Times URL and the state name and date are parameters that can be changed allowing this report to be run for other states/dates.

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Q1: Take a moment to reflect on the value of open data: How does easy access to historical and real-time environmental data shape our understanding of climate trends, resource management, and public health? What happens when this data disappears or becomes inaccessible? The role of independent archiving and collaborative stewardship has never been more critical in ensuring scientific progress and accountability.

It helps to understand patterns that may have not been recorded before. We are now able to have continuous data at all times, which helps shows trends, that couldn't be determined before. When this data becomes inaccessible, the ability to make accurate predictions decrease.

# Q2:Lets pretend it in January 1st, 2022. You are a data scientist for the state of Colorado Department of Public Health.

You’ve been tasked with giving a report to Governor Polis each morning about the most current COVID-19 conditions at the county level.

As it stands, the Colorado Department of Public Health maintains a watch list of counties that are being monitored for worsening corona virus trends. There are six criteria used to place counties on the watch list:

Doing fewer than 150 tests per 100,000 residents daily (over a 7-day average)
More than 100 new cases per 100,000 residents over the past 14 days…
25 new cases per 100,000 residents and an 8% test positivity rate
10% or greater increase in COVID-19 hospitalized patients over the past 3 days
Fewer than 20% of ICU beds available
Fewer than 25% ventilators available
Of these 6 conditions, you are in charge of monitoring condition number 2.

To do this job well, you should set up a reproducible framework to communicate the following in a way that can be updated every time new data is released (daily):

cumulative cases in the 5 worst counties
total NEW cases in the 5 worst counties
A list of safe counties
A text report describing the total new cases, total cumulative cases, and number of safe counties.
You should build this analysis in such a way that running it will extract the most current data straight from the NY-Times URL and the state name and date are parameters that can be changed allowing this report to be run for other states/dates.
```{r} 
#Download libraries,read in COVID-19 Data, and prepare data table for calculations
library(tidyverse) #data wrangling and visualization) 

library(flextable) #make nice tables)

library(zoo) #(rolling averages
library(patchwork)
library(sfd)
library(lubridate)
library(skimr)

url = 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv'
covid_raw<-read_csv(url)

#create my.date and my.state object. Set my date to "2022-02-01" and my state to "colorado"
my.date <- "2022-02-01" 
  class(my.date)
date<-my.date %>%
  as.Date()
class(date)
my.state <- "Colorado"

#Start by making a subset that limits (filter) the data to Colorado and add a new column (mutate) with the daily new cases using diff/lag by county (group_by). Do the same for new deaths as well.

#(Hint: you will need some combination of filter, group_by, arrange, mutate, diff/lag, and ungroup)

covid1 = covid_raw %>%
  filter(state == my.state) %>%
  group_by(county) %>%
  arrange(county,date) %>%
  mutate(
    new_cases = cases - lag(cases, order_by = date),
    new_deaths = deaths - lag(deaths, order_by = date)
  ) %>%
  ungroup()

head(covid1)
```

# filter the data to include only records for the date my.date, which is 2022-02-01. Using your subset, generate (2) tables. The first should show the 5 counties with the most CUMULATIVE cases, and the second should show the 5 counties with the most NEW cases. Remember to use your my.date object as a proxy for today’s date:
```{r}
filter(covid1, date == my.date) %>%
  slice_max(cases, n = 5) %>%
  select(Date = date, County = county, Cases = cases) %>%
  flextable() %>%
  set_caption(caption = "Top 5 Counties with the Most Cumulative Cases as of 2022-02-01")

filter(covid1, date == my.date) %>%
  slice_max(cases, n = 5) %>%
  select(Date = date, County = county, Cases = new_cases) %>%
  flextable() %>%
  set_caption(caption = "Top 5 Counties with the Most New Cases as of 2022-02-01")

```


# Q3: Normalizing Data
Raw count data can be deceiving given the wide range of populations in Colorado countries. To help us normalize data counts, we need supplemental population data to be added. Population data is offered by the Census. Please read in this data.
```{r}
pop_url <- 'https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/counties/totals/co-est2023-alldata.csv'

pop_covid_raw <-read_csv(pop_url)
```

# You notice that the COVID data provides a 5 digit character FIP code representing the state in the first 2 digits and the county in the last 3. In the population data, the STATE and COUNTY FIP identifiers are read in as numerics. To make these compatible we need to:

1.Convert the STATE numeric into a character forced to 2 digits with a leading 0 (when needed)
2.Convert the COUNTY numeric into a character forced to 3 digits with leading 0’s (when needed)
3.Create a FIP variable the STATE numeric into a character forced to 2 digits with a leading 0 (when needed)

# Given the above URL, and guidelines on string concatenation and formatting, read in the population data and (1) create a five digit FIP variable and only keep columns that contain “NAME” or “2021” (remember the tidyselect option found with ?dplyr::select). Additionally, remove all state level rows (e.g. COUNTY FIP == “000”)
```{r}
pop_covid1 <- pop_covid_raw %>%
  filter(COUNTY != "000") %>% 
  mutate(state_fips = sprintf("%02s", STATE), #for character use %02s
         county_fips = sprintf("%03s", COUNTY)) %>%
  mutate(fips_code = paste(state_fips, county_fips, sep = "")) %>% #taking state and county fips and combinging into new column
  select(fips_code, contains("NAME"), contains("2021")) #selecting only flips_code with all the name parameters and year 2021
```


# Now, explore the data … what attributes does it have, what are the names of the columns? Do any match the COVID data we have? What are the dimensions… In a few sentences describe the data obtained after modification:
```{r}
#Hint: names(), dim(), nrow(), str(), glimpse, skimr,…
# Explore the population data
names(pop_covid1)
dim(pop_covid1)
nrow(pop_covid1)
str(pop_covid1)
glimpse(pop_covid1)
skimr::data_cols(pop_covid1)
```
# The covid1 data set contains a tbl_df. tbl, and data.frame class, with 3,144 rows and 19 columns.
The names of the the columns are:
[1] "fips_code"             "STNAME"               
 [3] "CTYNAME"               "POPESTIMATE2021"      
 [5] "NPOPCHG2021"           "BIRTHS2021"           
 [7] "DEATHS2021"            "NATURALCHG2021"       
 [9] "INTERNATIONALMIG2021"  "DOMESTICMIG2021"      
[11] "NETMIG2021"            "RESIDUAL2021"         
[13] "GQESTIMATES2021"       "RBIRTH2021"           
[15] "RDEATH2021"            "RNATURALCHG2021"      
[17] "RINTERNATIONALMIG2021" "RDOMESTICMIG2021"     
[19] "RNETMIG2021"

The raw covid data matches some of the data in the pop_covid1 columns. Those columns are "fips_code", "STNAME", "CTYNAME".

The dimensions of the pop_covid1 data set are 3144 rows and 19 columns.


# What is the range of populations seen in Colorado counties in 2021?
```{r}
# Get the range of populations for Colorado counties in 2021

range(pop_covid1$POPESTIMATE2021)
```
# The min population of Colorado counties in 2021 is from 54 people and the max is 9,809,462 people.


# Join the population data to the Colorado COVID data and compute the per capita cumulative cases, per capita new cases, and per capita new deaths:
```{r}
covid1<-covid1 %>%
  rename(fips_code=fips) #rename fips column to match column name in pop_covid1
# Merge with the Covid1 data, which has state set to Co

merged_data <- covid1 %>%
  left_join(pop_covid1, by = "fips_code") #joining covid data on the left to the pop_covid 1

# Calculate per capita cumulative cases, per capita cases, and new deaths
merged_data <- merged_data %>%
  drop_na() %>%
  mutate(
    cumulative_cases_percapita = cases/ POPESTIMATE2021 * 100000,
    per_capita_new_cases = new_cases / POPESTIMATE2021 * 100000,
    per_capita_new_deaths = new_deaths / POPESTIMATE2021* 100000 )

head(merged_data)
```

# Generate (2) new tables. The first should show the 5 counties with the most cumulative cases per capita on 2021-01-01, and the second should show the 5 counties with the most NEW cases per capita on the same date. Your tables should have clear column names and descriptive captions.
(Hint: Use `flextable::flextable() and flextable::set_caption())
```{r}
# Table 1: Top 5 counties with the most cumulative cases per capita
top_cumulative_cases_per_capita <- merged_data %>%
  filter(date == "2021-01-01") %>%
  arrange(desc(cumulative_cases_percapita)) %>%
  select(county, cumulative_cases_percapita) %>%
  head(5)
  
 flextable::flextable(top_cumulative_cases_per_capita) %>%
  set_caption("Top 5 Counties with the Most Cumulative Cases Per Capita on 2021-01-01")
  
```
```{r}
# Table 2: Top 5 counties with the most new cases per capita
top_new_cases_per_capita <- merged_data %>%
  filter(date=="2021-01-01") %>%
  arrange(desc(per_capita_new_cases)) %>%
  select(county, per_capita_new_cases) %>%
  head(5)

 flextable::flextable(top_new_cases_per_capita) %>%
  set_caption("Top 5 Counties with the Most New Cases Per Capita on 2021-01-01")

```

# Q 4: Question 4: Rolling thresholds
Filter the merged COVID/Population data to only include the last 14 days. Remember this should be a programmatic request and not hard-coded. Then, use the group_by/summarize paradigm to determine the total number of new cases in the last 14 days per 100,000 people. Print a table of the top 5 counties, and, report the number that meet the watch list condition: “More than 100 new cases per 100,000 residents over the past 14 days…”

(Hint: Dates are numeric in R and thus operations like max min, -, +, >, and< work.)
```{r}
# Step 1-4: Filter, Summarize, Identify Watch List, and Display Results in One Flow

county_14d_new_cases <-merged_data %>%
  filter(date >= max(date) - 13) %>%
  group_by(county) %>%
  summarize(
    total_new_cases_14d = sum(new_cases, na.rm = TRUE),
    cases_per_100k_14d = (total_new_cases_14d / POPESTIMATE2021[1]) * 100000
  ) %>%
   filter(cases_per_100k_14d > 100) %>%  #need to filter by the total new cases
  arrange(desc(total_new_cases_14d))         #always filter before arrange 
  

# Display Top 5 Counties Table
county_14d_new_cases %>%
  head(5) %>%
  flextable::flextable() %>%
  set_caption("Top 5 Colorado Counties with Most New Cases Per Capita (Last 14 Days") %>%
  print()
   

```

# Watchlist
```{r}
#to make watch list
   watch_list <- filter(county_14d_new_cases,cases_per_100k_14d > 100) %>%
   head(5) %>%
     flextable::flextable() %>%
     set_caption("Top 5 Counties that have more than 100 new cases per 100,000 residents over the last 14 days") %>%
     print()
```


There are `r nrow(watch_list)` counties under watch.


# Q5: Given we are assuming it is February 1st, 2022. Your leadership has asked you to determine what percentage of deaths in each county were attributed to COVID last year (2021). You eagerly tell them that with the current Census data, you can do this!

From previous questions you should have a data.frame with daily COVID deaths in Colorado and the Census based, 2021 total deaths. For this question, you will find the ratio of total COVID deaths per county (2021) of all recorded deaths. In a plot of your choosing, visualize all counties where COVID deaths account for 20% or more of the annual death toll.

```{r}
#To extract a element of a date object in R
tmp.date = as.Date("2022-02-01")
lubridate::year(tmp.date)
lubridate::month(tmp.date)
lubridate::yday(tmp.date)


Colorado2021 <- covid1 %>%  #filter the data to 2022-02-01
  filter(date == tmp.date) 

#create new data frame with sum of new_deaths
county_total_deaths21 <- Colorado2021 %>%
  inner_join(pop_covid1, by = "fips_code") %>%
  mutate(death_ratio = (deaths / POPESTIMATE2021) * 100) %>%
  filter(death_ratio > .20)

  
#barplot of counties of COVID deaths 20% or greater than total deaths in 2021
ggplot(county_total_deaths21, aes(x = reorder(county, death_ratio), y = death_ratio)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() + 
  labs(
    title = "Counties Where COVID Deaths Account for 20% or More of Total Deaths in 2021",
    x = "County",
    y = "COVID Deaths as Percentage of Total Deaths"
  ) +
  theme_minimal()
```


# Q6: In this question, we are going to look at the story of 4 states and the impact scale can have on data interpretation. The states include: New York, Colorado, Alabama, and Ohio. Your task is to make a faceted bar plot showing the number of daily, new cases at the state level.
```{r}
# Step 1: Group the data to the state level and calculate daily new cases
state_covid <- covid_raw %>%
  group_by(date, state) %>%
  drop_na() %>%
  summarise(cases = sum(cases)) %>%
  filter(state %in% c('New York', 'Ohio', 'Colorado', 'Alabama')) %>%
  group_by(state) %>%
  mutate(new_cases = cases - lag(cases),
         roll = zoo:: rollmean(new_cases, k = 7, align = "right", fill = NA)) %>%
  ungroup()

```

# Using the modified data, make a facet plot of the daily new cases and the 7-day rolling mean. Your plot should use compelling geoms, labels, colors, and themes.
```{r}
# Create a simpler bar plot with the 7-day rolling mean
ggplot(state_covid, aes(x = date)) + 
  geom_col(aes(y = new_cases), fill = "blue", col= NA) +
  geom_line(aes(y = roll), col = "darkred", linewidth = 1) +
  theme_linedraw()+
  facet_wrap(~state, nrow = 2, scales = "free_y") +
  labs(title = "Cumulative COVID-19 Cases", 
       x = "Date", y = "Case Count")

```

# The story of raw case counts can be misleading. To understand why, lets explore the cases per capita of each state. To do this, join the state COVID data to the population estimates and calculate the newcases/totalpopulation. Additionally, calculate the 7-day rolling mean of the new cases per capita counts. This is a tricky task and will take some thought, time, and modification to existing code (most likely)!
```{r}
#create new state dataframe 
#Group by state and date to summarize data at the state level
pop_covid2_states<- pop_covid1 %>%
  group_by(STNAME) %>%  #Group by state and date to summarize data at the state level
summarise(state_population = sum(POPESTIMATE2021)) %>%
  inner_join(state_covid, by = c("STNAME" = "state")) %>% #join by ST and state from both dataframes
  mutate(per_capita_cases = new_cases / state_population) %>%
  group_by(STNAME) %>%
  drop_na() %>%
  mutate(roll = zoo::rollmean(per_capita_cases, k = 7, align = "right", fill = NA)
         ) %>%
  ungroup()

```


# Using the per capita data, plot the 7-day rolling averages overlying each other (one plot) with compelling labels, colors, and theme.
```{r}

#plot 7 day rolling averages overlying each other
ggplot(pop_covid2_states, aes(x = date)) +
  geom_line(aes(y = roll, col = STNAME), linewidth = 1) +
  theme_linedraw() +
  labs(title = "7-Day Rolling Mean of COVID-19 New Cases Per Capita by State",
       x = "Date", y = "Case Count")

```

# Briefly describe the influence scaling by population had on the analysis? Does it make some states look better? Some worse? How so?

 States that had lower populations populations, like Colorado and Alabama, may show higher per capita case rates even if their total case counts are smaller. It makes New York look like it had less cases even though New York has a bigger population and most likely had more cases. Colorado looks worse because it has a lower population, which makes the rolling mean a bigger number.
 
 
 # Q7: For our final task, we will explore our first spatial example! In it we will calculate the Weighted Mean Center of the COVID-19 outbreak in the USA to better understand the movement of the virus through time.
 
 # Please read in the data (readr::read_csv()); and join it to your raw COVID-19 data using the fips attributes
```{r}
# Step 1: Read the centroid data (latitude and longitude of county centers)
centroids_url <- "https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv"
centroids <- read_csv(centroids_url)



#Please read in the data (readr::read_csv()); and join it to your raw COVID-19 data using the fips attributes
spatial_join <- centroids %>%
 inner_join(covid_raw, by =c("fips"="fips")) %>%
  group_by(date) %>%
  summarise(weighted_lon= sum(LON*cases) / sum(cases),
            weighted_lat = sum(LAT*cases) / sum(cases),
            total_cases = sum(cases)) %>%
  arrange(date) %>%
  mutate(d = 1:n())

#add a month column
spatial_join <- spatial_join %>%
  mutate(month=format(as.Date(spatial_join$date), "%m"))


```

 
 # Plot the weighted mean center (aes(x = LNG, y = LAT)), colored by month, and sized by total cases for each day. These points should be plotted over a map of the USA states which can be added to a ggplot object with:
```{r}

ggplot(spatial_join) +
  # Map of the USA states
  borders("state", fill = "gray90", colour = "white") +
  # Plot the weighted mean centers (points) over the USA map
  geom_point(aes(x = weighted_lon, y = weighted_lat, colour =month, size = ""), alpha=.50) +
  theme_linedraw() +
  labs(
    title = "COVID-19 Weighted Mean Center of the USA",
    subtitle = "Colored by Month, Sized by Total Cases",
    x = "Longitude",
    y = "Latitude",
    color = "Month",
    size = "Total Cases"
  ) +
  theme(legend.position = "bottom")
```
 
 #In a few sentences, describe the movement of the COVID-19 weighted mean throughout the USA and possible drivers of its movement given your knowledge of the outbreak hot spots.
 
 It appears to have broken out mainly between Missouri and Arkansas during January-March. It was also spotted in the West, but lower cases than in the two other states. From Missouri it moved East early on and then drifted back West towards the spring/summer months. This could be because COVID was not known in the winter months, and by the time it was figured to be an epidemic many people had traveled. Also, during COVID, many people traveled to outdoor recreational areas in the spring months; this could explain why more people developed cases in the Missouri/ Arkansas area.
 
 
 # Q8:As extra credit, extend your analysis in problem three to also compute the weighted mean center of daily COVID deaths.

Make two plots next to each other (using patchwork) showing cases in red and deaths in navy. Once completed describe the differences in the plots and what they mean about the spatial patterns seen with COVID impacts.
```{r}
# Create the plot for COVID cases
colorado_join <- centroids %>%
 inner_join(covid1, by =c("fips"="fips_code")) %>%
  group_by(date) %>%
  summarise(weighted_lon= sum(LON*cases) / sum(cases),
            weighted_lat = sum(LAT*cases) / sum(cases),
            total_cases = sum(cases)) %>%
  arrange(date) %>%
  mutate(d = 1:n())


plot_covid_cases <- colorado_join %>%
  filter(date == my.date) %>%
  ggplot(aes(x = weighted_lon, y = weighted_lat, fill = total_cases , color="red")) +
  borders("state", fill = "gray90", colour = "white") +
   geom_point(aes(x = weighted_lon, y = weighted_lat, color="red", size =total_cases), alpha=.50) 
   # Rotate x labels for readability

# Create the plot for COVID deaths
colorado_join1 <- centroids %>%
 inner_join(covid1, by =c("fips"="fips_code")) %>%
  group_by(date) %>%
  summarise(weighted_lon= sum(LON*deaths) / sum(deaths),
            weighted_lat = sum(LAT*deaths) / sum(deaths),
            total_deaths = sum(deaths)) %>%
  arrange(date) %>%
  mutate(d = 1:n())

plot_covid_deaths <- colorado_join1 %>%
  filter(date == my.date) %>%
  ggplot(aes(x = weighted_lon, y = weighted_lat, fill = total_deaths , color="navy")) +
  borders("state", fill = "gray90", colour = "white") +
   geom_point(aes(x = weighted_lon, y = weighted_lat, color="navy", size =total_deaths), alpha=.50) 

#combine plots side by side using pathwork
plot_covid_cases | plot_covid_deaths

```

# Once completed describe the differences in the plots and what they mean about the spatial patterns seen with COVID impacts.

There appears to be more cases than there are deaths. It also seems like there are both more cases and deaths in the center of Colorado, which could be where the bigger cities are.
 