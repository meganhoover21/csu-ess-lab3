---
title: "lab 3: COVID-19"
subtitle: 'Ecosystem Science and Sustainability 330'
author:
  - name: Megan Hoover
    email: megan.hoover21@gmail.com
format: html
execute: 
  echo: true
output:
  html
    self contained:true
knitr:
  opts_chunk:
    collapse: true
    comment: "#>"
---

## Q2: Lets pretend it in January 1st, 2022. You are a data scientist for the state of Colorado Department of Public Health.

You’ve been tasked with giving a report to Governor Polis each morning about the most current COVID-19 conditions at the county level.

As it stands, the Colorado Department of Public Health maintains a watch list of counties that are being monitored for worsening corona virus trends. There are six criteria used to place counties on the watch list:

1.Doing fewer than 150 tests per 100,000 residents daily (over a 7-day average)
2.More than 100 new cases per 100,000 residents over the past 14 days…
3.25 new cases per 100,000 residents and an 8% test positivity rate
4.10% or greater increase in COVID-19 hospitalized patients over the past 3 days
5.Fewer than 20% of ICU beds available
6.Fewer than 25% ventilators available 
Of these 6 conditions, you are in charge of monitoring condition number 2.

To do this job well, you should set up a reproducible framework to communicate the following in a way that can be updated every time new data is released (daily):

1. cumulative cases in the 5 worst counties
2. total NEW cases in the 5 worst counties
3. A list of safe counties
4. A text report describing the total new cases, total cumulative cases, and number of safe counties.
You should build this analysis in such a way that running it will extract the most current data straight from the NY-Times URL and the state name and date are parameters that can be changed allowing this report to be run for other states/dates.

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Q1: Take a moment to reflect on the value of open data: How does easy access to historical and real-time environmental data shape our understanding of climate trends, resource management, and public health? What happens when this data disappears or becomes inaccessible? The role of independent archiving and collaborative stewardship has never been more critical in ensuring scientific progress and accountability.

It helps to understand patterns that may have not been recorded before. We are now able to have continuous data at all times, which helps shows trends, that couldn't be determined before. When this data becomes inaccessible, the ability to make accurate predictions decrease.

# Q2:Lets pretend it in January 1st, 2022. You are a data scientist for the state of Colorado Department of Public Health.

You’ve been tasked with giving a report to Governor Polis each morning about the most current COVID-19 conditions at the county level.

As it stands, the Colorado Department of Public Health maintains a watch list of counties that are being monitored for worsening corona virus trends. There are six criteria used to place counties on the watch list:

Doing fewer than 150 tests per 100,000 residents daily (over a 7-day average)
More than 100 new cases per 100,000 residents over the past 14 days…
25 new cases per 100,000 residents and an 8% test positivity rate
10% or greater increase in COVID-19 hospitalized patients over the past 3 days
Fewer than 20% of ICU beds available
Fewer than 25% ventilators available
Of these 6 conditions, you are in charge of monitoring condition number 2.

To do this job well, you should set up a reproducible framework to communicate the following in a way that can be updated every time new data is released (daily):

cumulative cases in the 5 worst counties
total NEW cases in the 5 worst counties
A list of safe counties
A text report describing the total new cases, total cumulative cases, and number of safe counties.
You should build this analysis in such a way that running it will extract the most current data straight from the NY-Times URL and the state name and date are parameters that can be changed allowing this report to be run for other states/dates.
```{r} 
#Download libraries,read in COVID-19 Data, and prepare data table for calculations
library(tidyverse) #data wrangling and visualization) 

library(flextable) #make nice tables)

library(zoo) #(rolling averages
library(patchwork)
library(sfd)

url = 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv'
covid_raw<-read_csv(url)

#create my.date and my.state object. Set my date to "2022-02-01" and my state to "colorado"
my.date <- "2022-02-01" 
  class(my.date)
date<-my.date %>%
  as.Date()
class(date)
my.state <- "Colorado"

#Start by making a subset that limits (filter) the data to Colorado and add a new column (mutate) with the daily new cases using diff/lag by county (group_by). Do the same for new deaths as well.

#(Hint: you will need some combination of filter, group_by, arrange, mutate, diff/lag, and ungroup)

covid1 = covid_raw %>%
  filter(state == my.state) %>%
  arrange(county, date) %>%
  group_by(county) %>%
  mutate(
    new_cases = cases - lag(cases, order_by = date),
    new_deaths = deaths - lag(deaths, order_by = date)
  ) %>%
  ungroup()

head(covid1)
```

# filter the data to include only records for the date my.date, which is 2022-02-01.
```{r}
top_cum_cases <- covid1 %>%
  filter(date== my.date) %>%
  arrange(desc(cases)) %>%
  select(county, cases) %>%
  head(5)

top_new_cases <- covid1 %>%
  filter(date== my.date) %>%
  arrange(desc(new_cases)) %>%
  select(county, new_cases) %>%
  head(5)
```

# Using your subset, generate (2) tables. The first should show the 5 counties with the most CUMULATIVE cases, and the second should show the 5 counties with the most NEW cases. Remember to use your my.date object as a proxy for today’s date:
```{r}
flex_top_cumcases = flextable(top_cum_cases)
  flex_top_cumcases %>%
  set_caption("Top 5 Counties with the Most Cumulative Cases as of 2022-02-01")
  
print(flex_top_cumcases)

flex_new_cases = flextable(top_new_cases)
flex_new_cases %>% 
  set_caption("Top 5 Counties with the Most New Cases as of 2022-02-01")

print(flex_new_cases)

```

# Q3: Normalizing Data
Raw count data can be deceiving given the wide range of populations in Colorado countries. To help us normalize data counts, we need supplemental population data to be added. Population data is offered by the Census. Please read in this data.
```{r}
pop_url <- 'https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/counties/totals/co-est2023-alldata.csv'

pop_covid_raw <-read_csv(pop_url)
```

# You notice that the COVID data provides a 5 digit character FIP code representing the state in the first 2 digits and the county in the last 3. In the population data, the STATE and COUNTY FIP identifiers are read in as numerics. To make these compatible we need to:

1.Convert the STATE numeric into a character forced to 2 digits with a leading 0 (when needed)
2.Convert the COUNTY numeric into a character forced to 3 digits with leading 0’s (when needed)
3.Create a FIP variable the STATE numeric into a character forced to 2 digits with a leading 0 (when needed)
```{r}
#convert state numeric into a character forced to 2 digits w/ a leading 0
#We can use the sprintf() function in base R to add leading zeros. The sprintf() function is powerful and versatile for string formatting.

#pop_covid1<- pop_covid_raw %>%
 # mutate(state_fips = sprintf("%02s", STATE), # This ensures state is two digits 
        # county_fips = sprintf("%03s", COUNTY)) %>%#This ensures county is 3 digits
      # mutate(fips_code = paste(state_fips, county_fips, sep = ""))                         

```


# Given the above URL, and guidelines on string concatenation and formatting, read in the population data and (1) create a five digit FIP variable and only keep columns that contain “NAME” or “2021” (remember the tidyselect option found with ?dplyr::select). Additionally, remove all state level rows (e.g. COUNTY FIP == “000”)
```{r}
pop_covid1 <- pop_covid_raw %>%
  filter(COUNTY != "000") %>% 
  mutate(state_fips = sprintf("%02s", STATE), #for character use %02s
         county_fips = sprintf("%03s", COUNTY)) %>%
  mutate(fips_code = paste(state_fips, county_fips, sep = "")) %>% #taking state and county fips and combinging into new column
  select(fips_code, contains("NAME"), contains("2021")) #selecting only flips_code with all the name parameters and year 2021
```

# Now, explore the data … what attributes does it have, what are the names of the columns? Do any match the COVID data we have? What are the dimensions… In a few sentences describe the data obtained after modification:
```{r}
#Hint: names(), dim(), nrow(), str(), glimpse, skimr,…
# Explore the population data
names(pop_covid1)
dim(pop_covid1)
nrow(pop_covid1)
str(pop_covid1)
glimpse(pop_covid1)
install.packages("skimr")
library(skimr)
skimr::data_cols(pop_covid1)
```
# The covid1 data set contains a tbl_df. tbl, and data.frame class, with 3,144 rows and 19 columns.
The names of the the columns are:
[1] "fips_code"             "STNAME"               
 [3] "CTYNAME"               "POPESTIMATE2021"      
 [5] "NPOPCHG2021"           "BIRTHS2021"           
 [7] "DEATHS2021"            "NATURALCHG2021"       
 [9] "INTERNATIONALMIG2021"  "DOMESTICMIG2021"      
[11] "NETMIG2021"            "RESIDUAL2021"         
[13] "GQESTIMATES2021"       "RBIRTH2021"           
[15] "RDEATH2021"            "RNATURALCHG2021"      
[17] "RINTERNATIONALMIG2021" "RDOMESTICMIG2021"     
[19] "RNETMIG2021"

The raw covid data matches some of the data in the pop_covid1 columns. Those columns are "fips_code", "STNAME", "CTYNAME".

The dimensions of the pop_covid1 data set are 3144 rows and 19 columns.

# What is the range of populations seen in Colorado counties in 2021?
```{r}
# Get the range of populations for Colorado counties in 2021
pop_covid1 %>%
  filter(str_detect(STNAME, "Colorado")) %>%
  summarise(min(POPESTIMATE2021),max(POPESTIMATE2021))

```
# The min population of Colorado counties in 2021 is 741 people and the max is 737, 287 people.

# Join the population data to the Colorado COVID data and compute the per capita cumulative cases, per capita new cases, and per capita new deaths:
```{r}
covid1<-covid1 %>%
  rename(fips_code=fips) #rename fips column to match column name in pop_covid1
# Merge with the Covid1 data, which has state set to Co

merged_data <- covid1 %>%
  left_join(pop_covid1, by = "fips_code") #joining covid data on the left to the pop_covid 1

# Calculate per per capita cumulative cases, per capita cases, and new deaths
merged_data <- merged_data %>%
  mutate(
    cumulative_cases_percapita = cases / POPESTIMATE2021 * 100000,
    newcases_per_capita = new_cases / POPESTIMATE2021 * 100000,
    new_deaths_per_capita = deaths / POPESTIMATE2021* 100000
  )
```

# Generate (2) new tables. The first should show the 5 counties with the most cumulative cases per capita on 2021-01-01, and the second should show the 5 counties with the most NEW cases per capita on the same date. Your tables should have clear column names and descriptive captions.
(Hint: Use `flextable::flextable() and flextable::set_caption())
```{r}
# Table 1: Top 5 counties with the most cumulative cases per capita
top_cumulative_cases_per_capita <- merged_data %>%
  filter(date == "2021-01-01") %>%
  arrange(desc(cumulative_cases_percapita)) %>%
  select(county, cumulative_cases_percapita) %>%
  head(5)
  
  top_cum_cases_per_cap_flex<- flextable(top_cumulative_cases_per_capita) %>%
  set_caption("Top 5 Counties with the Most Cumulative Cases Per Capita on 2021-01-01")
  
  print(top_cum_cases_per_cap_flex)
```
```{r}
# Table 2: Top 5 counties with the most new cases per capita
top_new_cases_per_capita <- merged_data %>%
  filter(date=="2021-01-01") %>%
  arrange(desc(newcases_per_capita)) %>%
  select(county, newcases_per_capita) %>%
  head(5)

top_new_cases_per_cap_flex<- flextable(top_new_cases_per_capita) %>%
  set_caption("Top 5 Counties with the Most New Cases Per Capita on 2021-01-01")
  
  print(top_new_cases_per_cap_flex)
```

# Q 4: Question 4: Rolling thresholds
Filter the merged COVID/Population data to only include the last 14 days. Remember this should be a programmatic request and not hard-coded. Then, use the group_by/summarize paradigm to determine the total number of new cases in the last 14 days per 100,000 people. Print a table of the top 5 counties, and, report the number that meet the watch list condition: “More than 100 new cases per 100,000 residents over the past 14 days…”

(Hint: Dates are numeric in R and thus operations like max min, -, +, >, and< work.)
```{r}
# Step 1-4: Filter, Summarize, Identify Watch List, and Display Results in One Flow
merged_14d<-merged_data %>%
  filter(date >= max(date) - 13) %>%
  group_by(county) %>%
  summarize(
    total_new_cases_14d = sum(new_cases, na.rm = TRUE),
    cases_per_100k_14d = (total_new_cases_14d / POPESTIMATE2021[1]) * 100000
  ) %>%
   filter(cases_per_100k_14d > 100) %>% #need to filter by the total new cases
  arrange(desc(total_new_cases_14d)) #always filter before arrange
  

#to make watch list
   watch_list <- filter(merged_14d,cases_per_100k_14d > 100)
   
   
    
    # Display Top 5 Counties Table
    merged_14d_flex<- merged_14d %>%
      slice_max(total_new_cases_14d, n=5 )%>% #tell it which column to sort
      flextable() %>%
      set_caption("Top 5 Colorado Counties with Most New Cases Per Capita (Last 14 Days)") %>%
      autofit() %>%
      print()

```


There are `r nrow(watch_list)` counties under watch.

# Q5: Given we are assuming it is February 1st, 2022. Your leadership has asked you to determine what percentage of deaths in each county were attributed to COVID last year (2021). You eagerly tell them that with the current Census data, you can do this!

From previous questions you should have a data.frame with daily COVID deaths in Colorado and the Census based, 2021 total deaths. For this question, you will find the ratio of total COVID deaths per county (2021) of all recorded deaths. In a plot of your choosing, visualize all counties where COVID deaths account for 20% or more of the annual death toll.

```{r}
covid.county_2021totaldeaths <-merged_data %>%
filter(date == my.date) %>%#filter to the my data 2021 date
 #create new data frame with sum of new_deaths
   mutate(death_ratio = deaths / DEATHS2021) %>%#this gives the ratio of cumulative deaths to deaths of 2021
filter(death_ratio >= .20)
  
#barplot of counties of COVID deaths 20% or greater than total deaths in 2021
ggplot(covid.county_2021totaldeaths, aes(x = reorder(county, death_ratio), y = death_ratio)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() + 
  labs(
    title = "Counties Where COVID Deaths Account for 20% or More of Total Deaths in 2021",
    x = "County",
    y = "COVID Deaths as Percentage of Total Deaths"
  ) +
  theme_minimal()
```


# Q6: In this question, we are going to look at the story of 4 states and the impact scale can have on data interpretation. The states include: New York, Colorado, Alabama, and Ohio. Your task is to make a faceted bar plot showing the number of daily, new cases at the state level.
```{r}
# Step 1: Group the data to the state level and calculate daily new cases
state_data <- covid_raw %>%
  filter(date== my.date)%>%
  group_by(state, date) %>% 
  summarize(
    totalcum_cases = sum(cases, na.rm = TRUE), # Sum the cumulative cases for each state
    totalcum_deaths = sum(deaths, na.rm = TRUE), # Sum the total deaths for each state
    .groups = "drop"
  ) %>%
  arrange(state, date) %>%
   mutate(
    daily_new_cases = totalcum_cases - lag(totalcum_cases, default = first(totalcum_cases)),  # Calculate daily new cases
    rolling_mean_7days = rollmean(daily_new_cases, k = 7, fill = NA, align = "center", na.rm= TRUE)  # 7-day rolling mean
  )

# Step 2: Filter for the four states (New York, Colorado, Alabama, Ohio)
 selected_states <- c("New York", "Colorado", "Alabama", "Ohio")
 

filtered_state_data <- state_data %>%
  filter(state %in% selected_states)

```

# Using the modified data, make a facet plot of the daily new cases and the 7-day rolling mean. Your plot should use compelling geoms, labels, colors, and themes.
```{r}
# Create a simpler bar plot with the 7-day rolling mean
ggplot(filtered_state_data, aes(x = date)) +
  # Bar plot for daily new cases
  geom_bar(aes(y = daily_new_cases), stat = "identity", fill = "skyblue", color = "black", width = 0.8) +
  # Line plot for 7-day rolling mean
  geom_line(aes(y = rolling_mean_7days), color = "darkred", size = 1) +
  # Facet by state
  facet_wrap(~ state, scales = "free_y") +
  # Labels and title
  labs(
    title = "Daily New COVID-19 Cases and 7-Day Rolling Mean by State",
    x = "Date",
    y = "Number of Cases",
    caption = "Bars represent daily new cases. The red line is the 7-day rolling mean."
  ) +
  # Simplified theme
  theme_minimal() 

```

# The story of raw case counts can be misleading. To understand why, lets explore the cases per capita of each state. To do this, join the state COVID data to the population estimates and calculate the newcases/totalpopulation. Additionally, calculate the 7-day rolling mean of the new cases per capita counts. This is a tricky task and will take some thought, time, and modification to existing code (most likely)!
```{r}
#create covid data frame with state info
covid2_states <- covid_raw %>%
  group_by(state, date) %>%  # Group by state and date to summarize data at the state level
  mutate(
    daily_new_cases = sum(cases) - lag(sum(cases), default = 0),  # Calculate daily new cases
    daily_new_deaths = sum(deaths) - lag(sum(deaths), default = 0)) %>% # Calculate daily new deaths
      ungroup()
  
  #create new pop data frame with state data
pop_covid2_states <- pop_covid_raw %>%
  mutate(state_fips = sprintf("%02s", STATE), #for character use %02s
         county_fips = sprintf("%03s", COUNTY)) %>%
  mutate(fips_code = paste(state_fips, county_fips, sep = "")) %>% #taking state and county fips and combinging into new column
  select(fips_code, contains("NAME"), contains("2021")) #selecting only flips_code with all the name parameters and year 2021
   

#rename fips column to match column name in pop_covid
covid2_states<- covid2_states %>%
  rename(fips_code=fips) #rename fips column to match column name in pop_covid2_states

#rename STNAME column to match covid2_states state name column
pop_covid2_states <- pop_covid2_states %>%
  rename(state=STNAME)


# Join state-level COVID data with the population data
merged_data2 <- covid2_states %>%
  left_join(pop_covid2_states, by= c("state"= "fip")) #joining covid data on the left to the pop_covid2_states


# Calculate daily new cases per capita
merged_data2 <- merged_data2 %>%
  mutate(
    new_cases_per_capita = daily_new_cases / POPESTIMATE2021* 100000) # Cases per 100,000 people

  
# Calculate 7-day rolling mean of new cases per capita
merged_data2<- merged_data2%>%
  group_by(state) %>%
  mutate(
    rollingmean_per_capita = rollmean(new_cases_per_capita, k = 7, fill = NA, align = "center")
  ) %>%
  ungroup()

#filter states 
merged_data2 <- merged_data2 %>%
  filter(state == "New York","Colorado","Alabama","Ohio")

```

# Using the per capita data, plot the 7-day rolling averages overlying each other (one plot) with compelling labels, colors, and theme.
```{r}
#filter the 4 states
filtered_state_data2<- merged_data2 %>%
  filter(state %in% selected_states)



# Plot the 7-day rolling averages of new cases per capita
ggplot(filtered_state_data2, aes(x = date, y = rollingmean_per_capita, color = state)) +
  geom_line(size = 1.2) +
  labs(
    title = "7-Day Rolling Mean of COVID-19 New Cases Per Capita by State",
    x = "Date",
    y = "7-Day Rolling Mean (New Cases Per 100,000 People)",
    color = "State",
    caption = "Each line represents the 7-day rolling mean of new cases per capita for each state."
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom",
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  ) +
  scale_color_brewer(palette = "Set1")
```

# Briefly describe the influence scaling by population had on the analysis? Does it make some states look better? Some worse? How so?
 States that had lower populations populations, like Colorado, may show higher per capita case rates even if their total case counts are smaller. It makes New York look like it had less cases even though New York has a bigger population and most likely had more cases. Colorado looks worse because it has a lower population, which makes the rolling mean a bigger number.
 
 
 # Q7: For our final task, we will explore our first spatial example! In it we will calculate the Weighted Mean Center of the COVID-19 outbreak in the USA to better understand the movement of the virus through time.
 
 # Please read in the data (readr::read_csv()); and join it to your raw COVID-19 data using the fips attributes
```{r}
# Step 1: Read the centroid data (latitude and longitude of county centers)
centroids_url <- "https://raw.githubusercontent.com/mikejohnson51/csu-ess-330/refs/heads/main/resources/county-centroids.csv"
centroids <- read_csv(centroids_url)

# Step 2: Join the COVID data with the centroids data based on the FIPS code
covid_centroid<- covid_raw %>%
  drop_na() %>%
  left_join(centroids, by = c("fips" = "fips"))

#3 Convert date column to Date type if it's not already
  covid_centroid %>%
    mutate(date = as.Date(date)) %>%
                                         # Calculate the month from the date
  mutate(month = format(date, "%m")) 
                                    
  
 # Calculate the Weighted Mean Center for each date
weighted_centers <- covid_centroid%>%
  drop_na() %>%
  # Group by date to calculate the weighted mean center for each day
  group_by(date) %>%
  summarise(
    weighted_lon = sum(LON* cases) / sum(cases),
    weighted_lat = sum(LAT * cases) / sum(cases),
    total_cases = sum(cases)
  ) %>%
  ungroup()

```
 
 #plot
```{r}
ggplot(weighted_centers) +
  # Map of the USA states
  borders("state", fill = "gray90", colour = "white") +
  # Plot the weighted mean centers (points) over the USA map
  geom_point(aes(x = weighted_lon, y = weighted_lat, color = format(date, "%m"), size = total_cases)) +
  theme_minimal() +
  labs(
    title = "COVID-19 Weighted Mean Center of the USA",
    subtitle = "Colored by Month, Sized by Total Cases",
    x = "Longitude",
    y = "Latitude",
    color = "Month",
    size = "Total Cases"
  ) +
  theme(legend.position = "bottom")
```
 
 #In a few sentences, describe the movement of the COVID-19 weighted mean throughout the USA and possible drivers of its movement given your knowledge of the outbreak hot spots.
 
 It appears to have broken out mainly between Missouri and Arkansas during January-March. It was also spotted in the West, but lower cases than in the two other states. From Missouri it moved East early on and then drifted back West towards the spring/summer months. This could be because COVID was not known in the winter months, and by the time it was figured to be an epidemic many people had traveled. Also, during COVID, many people traveled to outdoor recreational areas in the spring months; this could explain why more people developed cases in the Missouri/ Arkansas area.
 
 
 # Q8:As extra credit, extend your analysis in problem three to also compute the weighted mean center of daily COVID deaths.

Make two plots next to each other (using patchwork) showing cases in red and deaths in navy. Once completed describe the differences in the plots and what they mean about the spatial patterns seen with COVID impacts.
```{r}
#join previous merged data with the centroid data
problem8<- merged_data %>%
  rename(fips=fips_code) %>%
  drop_na() %>%
  left_join(centroids, by = c("fips" = "fips"))

problem8 %>%
    mutate(date = as.Date(date)) %>%
                                         # Calculate the month from the date
  mutate(month = format(date, "%m")) 


# Calculate the Weighted Mean Center for each date
weighted_centers_death <- problem8%>%
  drop_na() %>%
  # Group by date to calculate the weighted mean center for each day
  group_by(date) %>%
  summarise(
    weighted_lon = sum(LON* deaths) / sum(deaths),
    weighted_lat = sum(LAT * deaths) / sum(deaths),
    total_deaths = sum(deaths)
  ) %>%
  ungroup()


#make one for cases now
weighted_centers_cases <- problem8 %>%
  mutate(date = as.Date(date)) %>%
  mutate(month = format(date, "%m")) %>%
  group_by(date) %>%
  summarise(
    weighted_lon_cases = sum(LON * cases) / sum(cases),
    weighted_lat_cases = sum(LAT * cases) / sum(cases),
    total_cases = sum(cases)
  ) %>%
  ungroup()


# Plot for Cases (in red)
plot_cases <- ggplot(weighted_centers_cases) +
  borders("state", fill = "gray90", colour = "white") +
  geom_point(aes(x = weighted_lon_cases, y = weighted_lat_cases, color ="red", 
             size =total_cases)) +
  theme_minimal() +
  labs(
    title = "COVID-19 Weighted Mean Center - Cases",
    subtitle = "Colored by Total Cases",
    x = "Longitude",
    y = "Latitude",
    color = "Total Cases"
  ) +
  theme(legend.position = "bottom")

# Plot for Deaths (in navy)
plot_deaths <- ggplot(weighted_centers_death) +
  borders("state", fill = "gray90", colour = "white") +
  geom_point(aes(x = weighted_lon, y = weighted_lat, color = "navy", 
            )) +
  theme_minimal() +
  labs(
    title = "COVID-19 Weighted Mean Center - Deaths",
    subtitle = "Colored by Total Deaths",
    x = "Longitude",
    y = "Latitude",
    color = "total_deaths") +
  theme(legend.position = "bottom")

plot_cases | plot_deaths

p1 <- ggplot(weighted_centers_cases, aes(weighted_lon_cases, weighted_lat_cases)) + geom_point()
p2 <- ggplot(weighted_centers_death, aes(weighted_lon, weighted_lat)) + geom_point()

p1|p2
```


# Once completed describe the differences in the plots and what they mean about the spatial patterns seen with COVID impacts.

There appears to be more cases between -105 to -104.75 degrees longitude, and there also seems to be more deaths in that range as well. It also seems like there are more cases than there are deaths though. 
 